# Projet-Graph-neural-networks

## Overview

This project is an exploration of **Graph Neural Networks (GNNs)**, a specialized class of deep learning models designed to operate on non-Euclidean data structures (graphs). The project aims to cover the fundamental concepts of graph theory, GNN mechanics, and their application to a real-world problem: predicting molecular properties using the **ZINC chemical compounds dataset** via the **PyTorch Geometric** library. GNN are most often used in chemistry problems to predict cehmical properties, like the odour of a molecule based on its chemical map or the physiological role of a protein. But they are also useful to modelize any network : one could predict the traffic at one crossroad knowing the number of cars at other points of the road network. Other examples are given in the report.

A molecule is constituted of different atoms, bounded by a phenomenon called covalent bonds. This phenomenon comes from atoms sharing electrons. Mathematically, one can represent a molecule via a graph, by considering atoms as vertices and their covalent bonds as edges. This is precisely what is done in the ZINC database. Each observation consists of an ordinated list of atoms identified by their atomic number, paired with an adjacency matrix describing the existing covalent bonds between the different atoms. The atomic numbers are the feature and the adjacency matrix describes the topology of our graph. The predicted continuous variable logP, or octanol-water partition coefficient, is a measure of how hydrophilic or hydrophobic a molecule is (per https://www.biotage.com/blog/what-is-the-role-of-logp-in-sample-prep-methods).

To perform predictions, we train a rudimentary Graph Neural Network, constituted of two layers of Graph Isomorphism Network and one layer of Global Mean pooling. The final layer is, as is usual for Convolution Neural Networks, constituted of a standard fully connected layer, outputting logP predictions. ReLU is used after each convolution as an activation function. The training is done with 100 epochs, using the Mean absolute error loss which is standard on ZINC. To use the script, the user must modify the "root" parameter of the second chunk to its working directory. Data is then downloaded on the user's computer.

After 100 epochs, the model achieves a MAE of 0.6655 on the test dataset. For reference, the variable logP has values comprised in [-62.138;4.519], with an average of 0 and a standard deviation of 2.049 on the training set. This score exceeds our expectations, as the model is kept simple and the number of epochs low in order to keep computing time reasonable. The pooling module would be the one thing to refine further in order to get better results, as global mean pooling does not take into account connexions between vertices, missing potentialy crucial information in the process.

